# -*- coding: utf-8 -*-
"""ВКР_Иванова Д.А.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TDuXtSTA23s0CA6QH3NTLqKMQDM-v0UQ

Итоговый проект слушателя курса Аналитик данных (Data scientist)

Выполнила Иванова Дарья

Цель:

1.   Провести предварительную обработку датасета
2.   Построить модель прогнозирования следующих целевых признаков:
     *   модуль упругости при растяжении;
     *   модуль прочности при растяжении.
3.   Применить миниум 3 алгоритма машинного обучения(включая искусственные нейронные сети) и выбрать лучший из них
4.   Сохранить обученную модель для последующего применения в приложении

Импорт необходимых библиотек
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pickle
import tensorflow as tf
import sklearn

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, Normalizer
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.models import Sequential 
from tensorflow.keras.optimizers import Adam

#Используемая версия tensorflow
tf.__version__

print('The scikit-learn version is {}.'.format(sklearn.__version__))

"""Загрузка данных"""

#Загружаем первый датасет (базальтопластик) и посмотрим на названия столбцов
df_bp = pd.read_excel(r"/content/sample_data/X_bp.xlsx")
df_bp.shape

#Удаляем первый неинформативный столбец
df_bp.drop(['Unnamed: 0'], axis=1, inplace=True)
#Посмотрим на первые 5 строк первого датасета и убедимся, что первый столбец удалился
df_bp.head()

# Проверим размерность первого файла
df_bp.shape

# Загружаем второй датасет (углепластик) 
df_nup = pd.read_excel(r"/content/sample_data/X_nup.xlsx")
df_nup.shape

#Удаляем первый неинформативный столбец
df_nup.drop(['Unnamed: 0'], axis=1, inplace=True)
#Посмотрим на первые 5 строк второго датасета и убедимся, что и здесь не нужный первый столбец успешно удалился
df_nup.head()

# Проверим размерность второго файла
df_nup.shape

#Объединим по индексу, тип объединения INNER, смотрим итоговый датасет

# Понимаем, что эти два датасета имеют разный объем строк. 
# Но наша задача собрать исходные данные файлы в один, единый набор данных. 
# По условию задачи объединяем их по типу INNER. 
df = df_bp.merge(df_nup, left_index = True, right_index = True, how = 'inner')
df.head().T

#Посмотрим количество колонок и столбцов
df.shape
# Итоговый датасет имеет 13 столбцов и 1023 строки, 17 строк из таблицы X_nup было отброшено,т.е часть данных удалена на начальном этапе исследования.

"""Разведочный анализ данных"""

#Общая информация о датасете
df.info()

#Описательная статистика датасета
df.describe().T

#Количество пропусков по каждому столбцу датасета
df.isna().sum()

#Количество уникальных значений по каждому столбцу датасета
df.nunique()

#Анализ показал, что столбец "Угол нашивки, град" имеет всего 2 уникальных значения,
# что позволяет нам рассматривать его значения как категориальный тип.

#Переведем данные столбца в категориальный тип
le = LabelEncoder()
df['Угол нашивки, град'] = le.fit_transform(df['Угол нашивки, град'].values)
df['Угол нашивки, град']

"""Визуализация характеристик датасета

Графический исследовательский анализ данных может помочь визуально обнаружить присутствие выбросов, выяснить распределение данных и уловить взаимосвязи между признаками.
"""

#Смотрим на характер распределния признаков датасета
fig, ax = plt.subplots(figsize=(30,15))
df.plot(kind='kde', ax=ax)

#На данном графике сложно что либо разобрать всвязи с тем, что все признаки имеют различные единицы измерения и находятся
# на разных численных интервалах.

#Гистограммы позволят определить характер распределения каждого из параметров
df.hist(figsize = (30, 30), color='darkblue')
plt.show()

#Распределение данных каждого из параметров, за исключением угла нашивки, близко к нормальному

#Матрица графиков рассеяния позволит визуализировать попарные взаимосвязи между разными признаками в наборе данных
sns.set_style
sns.pairplot(df, height=2.5)
plt.tight_layout()
plt.show()

#Корреляционная матрица представляет собой квадратную матрицу, 
#содержащую значения коэффициента корреляции, 
#который измеряет линейную зависимость между парой признаков
sns.set(font_scale=2)
fig, ax = plt.subplots(figsize=(15, 10))
heatMap = sns.heatmap(df.corr(), cbar=True, annot=True, square=True, fmt='.2f', cmap='YlGnBu', linewidths=1, linecolor='white', annot_kws={'size': 15})
plt.xticks(rotation=45, ha='right')
plt.show()

#Изучив данные графики можно сделать вывод о слабой корреляции между данными, так как практически везде она стремится к нулю.

#Боксплот (ящик с усами)
plt.figure(figsize=(15,10))
ax = sns.boxplot(data=df, orient="h")
plt.xticks(rotation=45, ha='right')
plt.xlabel('Условные единицы измерения')
plt.show() 

#Данные имеют различные единицы измерения и соотвественно находятся в разных числовых диапазонах, 
# что также хорошо видно на графике боксплот. Это говорит нам о необходимости обработки данных с целью приведения их
# к единому формату для лучшей визуализации выбросов.

"""Очистка датасета от выбросов"""

#Для лучшей визуализации выбросов необходимо привести данные к единому диапазону: [0..1].
# Для этой цели будем использовать MinMaxScaler. Так как MinMaxScaler не меняет исходного распределения данных, а лишь приводит
# данные к единому интервалу не будет лишним применить StandardScaler, чтобы задать единую среднюю точку в нуле для всего
# датасета, со стандартным отклонением, равным единице.


# Напишем функцию преобразования данных, чтобы иметь возможность в любой момент преобразовать данные для визуализации.
def processing(data):
    mms = MinMaxScaler()
    sc = StandardScaler()
    result = mms.fit_transform(data)
    result = sc.fit_transform(result)
    data = pd.DataFrame(result, columns=data.columns)
    return data

#Преобразуем датасет
df_mms_sc = processing(df)

#Смотрим на преобразованный датасет
df_mms_sc.head()

df_mms_sc.describe().T

#Ящик с усами для преобразованных данных
plt.figure(figsize=(15,10))
ax = sns.boxplot(data=df_mms_sc, orient="h")
plt.show()

#График боксплот, построенный на масштабированных данных, гораздо лучше отображает необходимость удаления выбросов.

#Для удаления выбросов существует 2 основных метода - метод 3-х сигм и межквартильных расстояний. Сравним эти 2 метода.
metod_3s = 0
metod_iq = 0
count_iq = [] # Список, куда записывается количество выбросов по каждой колонке датафрейма методом.
count_3s = [] # Список, куда записывается количество выбросов по каждой колонке датафрейма.
for column in df:
    d = df.loc[:, [column]]
    # методом 3-х сигм
    zscore = (df[column] - df[column].mean()) / df[column].std()
    d['3s'] = zscore.abs() > 3
    metod_3s += d['3s'].sum()
    count_3s.append(d['3s'].sum())
    print(column,'3s', ': ', d['3s'].sum())

    # методом межквартильных расстояний
    q1 = np.quantile(df[column], 0.25)
    q3 = np.quantile(df[column], 0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    d['iq'] = (df[column] <= lower) | (df[column] >= upper)
    metod_iq += d['iq'].sum()
    count_iq.append(d['iq'].sum())
    print(column, ': ', d['iq'].sum())
print('Метод 3-х сигм, выбросов:', metod_3s)
print('Метод межквартильных расстояний, выбросов:', metod_iq)

#Создадим переменную со списком всех параметров, в которых есть выбросы
df.columns
column_list_drop = ["Соотношение матрица-наполнитель",
                 "Плотность, кг/м3",
                 "модуль упругости, ГПа",
                 "Количество отвердителя, м.%",
                 "Содержание эпоксидных групп,%_2",
                 "Температура вспышки, С_2",
                 "Поверхностная плотность, г/м2",
                 "Модуль упругости при растяжении, ГПа",
                 "Прочность при растяжении, МПа",
                 "Потребление смолы, г/м2",
                 "Шаг нашивки",
                 "Плотность нашивки"]

# Исключим выбросы, очистим данные от выбросов методом межквартильного расстояния (далее 1,5 межквартильных размахов)
for i in column_list_drop:
    q75, q25 = np.percentile(df.loc[:,i], [75,25])
    intr_qr = q75 - q25
    max = q75 + (1.5 * intr_qr)
    min = q25 - (1.5 * intr_qr)
    df.loc[df[i] < min, i] = np.nan
    df.loc[df[i] > max, i] = np.nan

# Метод 3-х сигм
m_3s = pd.DataFrame(index=df.index)
for column in df:
    zscore = (df[column] - df[column].mean()) / df[column].std()
    m_3s[column] = (zscore.abs() > 3)
df = df[m_3s.sum(axis=1)==0]
df.shape

#Преобразуем датасет для лучшей визуализации
df_mms_sc = processing(df)

plt.figure(figsize=(15,10))
ax = sns.boxplot(data=df_mms_sc, orient="h")
plt.xticks(rotation=45, ha='right')
plt.show()
#На графике видно, что большинство выбросов было успешно удалено из датасета, но столбцы
# "Модуль упругости", "Прочность при растяжении", "Потребление смолы" 
# и "Плотность нашивки" все еще имеют значения, похожие на выбросы. В данном случае не будем считать их таковыми, так как
# они не превышают границы трех сигм нормального распределения.

#Смотрим, что осталось от датасета после очистки от выбросов
df.shape
#Размерность датасета после очистки не изменилась. Из этого следует, что выбросы являлись отдельными ячейками каждой строки, 
# а не вся строка целиком.

df

#Проверяем количество пропусков
df.isna().sum()

#Удаляем пропуски в датасете.
df.dropna(axis=0, inplace=True)

#Снова проверяем количество пропусков
df.isna().sum()

#Смотрим, что осталось от датасета после чистки
df.shape
#После обработки датасета количество строк, подлежащих передаче моделям на обучение соствляет 936 строк и 13 столбцов.

"""Графический анализ обработанных данных

"""

#Распределение признаков преобразованного датасета
fig, ax = plt.subplots(figsize=(30,15))
df_mms_sc.plot(kind='kde', ax=ax)

#Вызовем функцию преобразования еще раз, на всякий случай
df_mms_sc = processing(df)

plt.figure(figsize=(15,10))
ax = sns.boxplot(data=df_mms_sc, orient="h")
plt.xticks(rotation=45, ha='right')
plt.show()
#После удаления строк с пустыми значениями распределение признаков стало чуть более чувствительным к "выбросам". Но, несмотря
# на это, "выделяющиеся" значения все еще можно не считать "выбросами", так как они продолжают находиться на интервале
# трех сигм стандартного отклонения.

"""Обучение моделей"""

#Для лучшей сходимости алгоритмов машинного обучения данные необходимо нормализовать.
normalizer = Normalizer()
result = normalizer.fit_transform(df)
df_norm = pd.DataFrame(result, columns = df.columns)

df_norm.head()

df_norm.describe().T

#Прогнозируемые величины  "Модуль упругости при растяжении" и "Модуль прочности при растяжении", а также все признаки, 
# за исключением столбца "Угол нашивки", являются непрерывными. Следовательно, для решения задачи такого типа будем
# использовать различные модели регрессии. Но так как в данных нет явной корреляции между 
# признаками, регрессионная модель может не дать хорошего прогноза. В таком случае можно использовать более сложные модели, 
# способные выявить взаимосвязь между признаками. Для таких целей хорошо подойдет модель случайного леса. В виду большого 
# числа признаков, находящихся в разных измерениях неплохим выбором будет метод k-ближайших соседей. По условию задачи,
# необходимо реализовать многослойную нейронную сеть, которая также должна хорошо обобщиться на многомерном датасете, 
# попытавшись выявить новые зависимости между признаками.

#Для начала обучения выделим из датасета целевые признаки, которые будем прогнозировать, а также разобьем его на обучающий
# и тестовый набор.

#Для упрощения реализации задачи объединим целевые признаки в единый датафрейм.
train = df_norm.drop(['Модуль упругости при растяжении, ГПа', 'Прочность при растяжении, МПа'], axis=1)
target = df_norm[['Модуль упругости при растяжении, ГПа', 'Прочность при растяжении, МПа']]

#Разбиение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(
    train,
    target,
    test_size=0.2,
    random_state=1)

#Проверяем размерность получившихся выборок
print(X_train.shape)
print(X_test.shape)

print(y_train.shape)
print(y_test.shape)

#Размерности совпадают

#Количество прогнозируемых признаков
y_train.shape[1]

#Для оценки качества моделей будем использовать метрику r^2(R в квадрате). Она представляет собой долю вариации 
# зависимой переменной, которая предсказуема из независимой переменной (переменных).

#Создадим хранилище для оценок, чтобы в дальнейшем выбрать лучшую модель
scores = {}  #Словарь коэффициентов детерминации для каждой модели

"""Линейная регрессия"""

#Линейная регрессия является классическим и наиболее известным алгоритмом регрессии. Однако, линейная регрессия 
# часто неприменима к реальным данным из-за слишком ограниченных возможностей.

lr = LinearRegression()

y_pred = []  #Список предиктов по каждому признаку. Используется для визуализации обучения
lr_scores = {}  #Словарь коэффициентов детерминации для линейной регрессии

#Обучаем модель для предсказания признака "Модуль упругости при растяжении"
lr.fit(X_train, y_train.iloc[:, 0])
y_pred.append(lr.predict(X_test))
r2_score_feature_1 = r2_score(y_test.iloc[:, 0], y_pred[0])
print(f'Коэффициент детерминации для признака "{y_train.columns[0]}": {r2_score(y_test.iloc[:, 0], y_pred[0])}')
lr_scores["lr_r2_score_feature_1"] = r2_score_feature_1

#Обучаем модель для предсказания признака "Прочность при растяжении, МПа"
lr.fit(X_train, y_train.iloc[:, 1])
y_pred.append(lr.predict(X_test))
r2_score_feature_2 = r2_score(y_test.iloc[:, 1], y_pred[1])
print(f'Коэффициент детерминации для признака "{y_train.columns[1]}": {r2_score(y_test.iloc[:, 1], y_pred[1])}')
lr_scores["lr_r2_score_feature_2"] = r2_score_feature_2

scores['LinearRegression'] = lr_scores

#Проверяем работу словаря
scores

#Выведем коэффициент детерминации для первого признака
scores['LinearRegression']['lr_r2_score_feature_1']

#Визуализируем полученный результат для обоих признаков.
fig, ax = plt.subplots(figsize=(15, 10))
ax.set_title('Линейная регрессия')
ax.bar(y_train.columns[0], scores['LinearRegression']['lr_r2_score_feature_1'], color = 'blue', width = 0.5)
ax.bar(y_train.columns[1], scores['LinearRegression']['lr_r2_score_feature_2'], color = 'green', width = 0.5)

#Визуализириуем качество обобщения модели на тестовых данных для каждого признака
fig, axs = plt.subplots(2, figsize=(25, 20))
for i in range(2):
    axs[i].set_title('Линейная регрессия')
    axs[i].plot(y_pred[i], label="Прогнозные значения", color='blue')
    axs[i].plot(y_test.iloc[:, i].values, label="Тестовые значения", color='green')
    axs[i].set(xlabel="Количество наблюдений")
    axs[i].set(ylabel=y_train.columns[i])
    axs[i].legend(loc='best')

"""LASSO-регрессия"""

#LASSO, Least Absolute Shrinkage and Selection Operator) — это вариация линейной регрессии. 
# LASSO использует сжатие коэффициентов (shrinkage) - процесс, в котором значения данных приближаются к центральной точке.

LCV = LassoCV()
y_pred = []  #Список предиктов по каждому признаку. Используется для визуализации обучения
LCV_scores = {}  #Словарь коэффициентов детерминации для LASSO регрессии

#Обучаем модель для предсказания признака "Модуль упругости при растяжении"
LCV.fit(X_train, y_train.iloc[:, 0])
y_pred.append(LCV.predict(X_test))
r2_score_feature_1 = r2_score(y_test.iloc[:, 0], y_pred[0])
print(f'Коэффициент детерминации для признака "{y_train.columns[0]}": {r2_score(y_test.iloc[:, 0], y_pred[0])}')
LCV_scores["LCV_r2_score_feature_1"] = r2_score_feature_1

#Обучаем модель для предсказания признака "Прочность при растяжении, МПа"
LCV.fit(X_train, y_train.iloc[:, 1])
y_pred.append(LCV.predict(X_test))
r2_score_feature_2 = r2_score(y_test.iloc[:, 1], y_pred[1])
print(f'Коэффициент детерминации для признака "{y_train.columns[1]}": {r2_score(y_test.iloc[:, 1], y_pred[1])}')
LCV_scores["LCV_r2_score_feature_2"] = r2_score_feature_2

scores['LassoCV'] = LCV_scores

#Визуализируем полученный результат для обоих признаков.
fig, ax = plt.subplots(figsize=(15, 10))
ax.set_title('LASSO регрессия')
ax.bar(y_train.columns[0], scores['LassoCV']['LCV_r2_score_feature_1'], color = 'blue', width = 0.5)
ax.bar(y_train.columns[1], scores['LassoCV']['LCV_r2_score_feature_2'], color = 'green', width = 0.5)

#Визуализириуем качество обобщения модели на тестовых данных для каждого признака
fig, axs = plt.subplots(2, figsize=(25, 20))
for i in range(2):
    axs[i].set_title('LASSO регрессия')
    axs[i].plot(y_pred[i], label="Прогнозные значения", color='blue')
    axs[i].plot(y_test.iloc[:, i].values, label="Тестовые значения", color='green')
    axs[i].set(xlabel="Количество наблюдений")
    axs[i].set(ylabel=y_train.columns[i])
    axs[i].legend(loc='best')

"""Гребневая регрессия"""

#Гребневая регрессия или ридж-регрессия очень похожа на регрессию LASSO в том, что она применяет сжатие. 
# Однако самое большое различие между ними в том, что гребневая регрессия использует регуляризацию L2, 
# то есть ни один из коэффициентов не становится нулевым, как это происходит в регрессии LASSO.

ridge = RidgeCV()

y_pred = []  #Список предиктов по каждому признаку. Используется для визуализации обучения
ridge_scores = {}  #Словарь коэффициентов детерминации для RIDGE регрессии

#Обучаем модель для предсказания признака "Модуль упругости при растяжении"
ridge.fit(X_train, y_train.iloc[:, 0])
y_pred.append(ridge.predict(X_test))
r2_score_feature_1 = r2_score(y_test.iloc[:, 0], y_pred[0])
print(f'Коэффициент детерминации для признака "{y_train.columns[0]}": {r2_score(y_test.iloc[:, 0], y_pred[0])}')
ridge_scores["ridge_r2_score_feature_1"] = r2_score_feature_1

#Обучаем модель для предсказания признака "Прочность при растяжении, МПа"
ridge.fit(X_train, y_train.iloc[:, 1])
y_pred.append(ridge.predict(X_test))
r2_score_feature_2 = r2_score(y_test.iloc[:, 1], y_pred[1])
print(f'Коэффициент детерминации для признака "{y_train.columns[1]}": {r2_score(y_test.iloc[:, 1], y_pred[1])}')
ridge_scores["ridge_r2_score_feature_2"] = r2_score_feature_2

scores['Ridge'] = ridge_scores

#Визуализируем полученный результат для обоих признаков.
fig, ax = plt.subplots(figsize=(15, 10))
ax.set_title('Гребневая регрессия')
ax.bar(y_train.columns[0], scores['Ridge']['ridge_r2_score_feature_1'], color = 'blue', width = 0.5)
ax.bar(y_train.columns[1], scores['Ridge']['ridge_r2_score_feature_2'], color = 'green', width = 0.5)

#Визуализириуем качество обобщения модели на тестовых данных для каждого признака
fig, axs = plt.subplots(2, figsize=(25, 20))
for i in range(2):
    axs[i].set_title('Гребневая регрессия')
    axs[i].plot(y_pred[i], label="Прогнозные значения", color='blue')
    axs[i].plot(y_test.iloc[:, i].values, label="Тестовые значения", color='green')
    axs[i].set(xlabel="Количество наблюдений")
    axs[i].set(ylabel=y_train.columns[i])
    axs[i].legend(loc='best')

"""ElasticNet регрессия"""

#ElasticNet стремится объединить лучшее из гребневой регрессии и регрессии лассо, комбинируя регуляризацию L1 и L2.

elastic = ElasticNetCV()

y_pred = []
elastic_scores = {}  #Список коэффициентов детерминации для ElasticNet регрессии

#Обучаем модель для предсказания признака "Модуль упругости при растяжении"
elastic.fit(X_train, y_train.iloc[:, 0])
y_pred.append(elastic.predict(X_test))
r2_score_feature_1 = r2_score(y_test.iloc[:, 0], y_pred[0])
print(f'Коэффициент детерминации для признака "{y_train.columns[0]}": {r2_score(y_test.iloc[:, 0], y_pred[0])}')
elastic_scores["elastic_r2_score_feature_1"] = r2_score_feature_1

#Обучаем модель для предсказания признака "Прочность при растяжении, МПа"
elastic.fit(X_train, y_train.iloc[:, 1])
y_pred.append(elastic.predict(X_test))
r2_score_feature_2 = r2_score(y_test.iloc[:, 1], y_pred[1])
print(f'Коэффициент детерминации для признака "{y_train.columns[1]}": {r2_score(y_test.iloc[:, 1], y_pred[1])}')
elastic_scores["elastic_r2_score_feature_2"] = r2_score_feature_2

scores['ElasticNetCV'] = elastic_scores

#Визуализируем полученный результат для обоих признаков.
fig, ax = plt.subplots(figsize=(15, 10))
ax.set_title('ElasticNet регрессия')
ax.bar(y_train.columns[0], scores['ElasticNetCV']['elastic_r2_score_feature_1'], color = 'blue', width = 0.5)
ax.bar(y_train.columns[1], scores['ElasticNetCV']['elastic_r2_score_feature_2'], color = 'green', width = 0.5)

#Визуализириуем качество обобщения модели на тестовых данных для каждого признака
fig, axs = plt.subplots(2, figsize=(25, 20))
for i in range(2):
    axs[i].set_title('ElasticNet регрессия')
    axs[i].plot(y_pred[i], label="Прогнозные значения", color='blue')
    axs[i].plot(y_test.iloc[:, i].values, label="Тестовые значения", color='green')
    axs[i].set(xlabel="Количество наблюдений")
    axs[i].set(ylabel=y_train.columns[i])
    axs[i].legend(loc='best')

"""Случайный лес"""

#Из-за специфической и высокодисперсной природы регрессии просто как задачи машинного обучения, 
# регрессоры дерева решений следует тщательно обрезать. Для поиска оптимальной глубины дерева необходимо применить
# инструмент оптимизации поиска параметров GridSearchCV. Данный метод потребляет значительное количество ресурсов и 
# его следует применять не для каждой задачи.

# Параметры, подаваемые на перекрестную проверку методу поиска по сетке
param_grid = { 'n_estimators': [100, 150, 200, 250, 300, 400, 500],
              'max_depth': [5, 7, 10, 15],
              'criterion': ['squared_error'] }

#Поиск лучшей комбинации гиперпараметров
clf = GridSearchCV(RandomForestRegressor(), param_grid, cv=10, 
                   n_jobs=-1, verbose=1)

#Для первого признака
clf.fit(X_train, y_train.iloc[:, 0])
print(f"Лучшие параметры для предсказания признака 'Модуль упругости при растяжении' {clf.best_params_}")

#Для второго признака
clf.fit(X_train, y_train.iloc[:, 1])
print(f"Лучшие параметры для предсказания признака 'Прочность при растяжении, МПа' {clf.best_params_}")

#Наилучшими параметрами для 
# предсказания признака 'Модуль упругости при растяжении' являются:
#  -количество деревьев: 100
#  -глубина дерева: 7
# для предсказания признака 'Прочность при растяжении, МПа' являются:
#  -количество деревьев: 500
#  -глубина дерева: 15

y_pred = []
rfr_scores = {}  #Список коэффициентов детерминации для случайного леса

#Обучаем модель для предсказания признака "Модуль упругости при растяжении"


rfr = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=1)
rfr.fit(X_train, y_train.iloc[:, 0])
y_pred.append(rfr.predict(X_test))
r2_score_feature_1 = r2_score(y_test.iloc[:, 0], y_pred[0])
print(f'Коэффициент детерминации для признака "{y_train.columns[0]}": {r2_score(y_test.iloc[:, 0], y_pred[0])}')
rfr_scores["rfr_r2_score_feature_1"] = r2_score_feature_1

#Обучаем модель для предсказания признака "Прочность при растяжении, МПа"
rfr = RandomForestRegressor(n_estimators=500, max_depth=15, random_state=1)
rfr.fit(X_train, y_train.iloc[:, 1])
y_pred.append(rfr.predict(X_test))
r2_score_feature_2 = r2_score(y_test.iloc[:, 1], y_pred[1])
print(f'Коэффициент детерминации для признака "{y_train.columns[1]}": {r2_score(y_test.iloc[:, 1], y_pred[1])}')
rfr_scores["rfr_r2_score_feature_2"] = r2_score_feature_2

scores['RandomForestRegressor'] = rfr_scores

#Визуализируем полученный результат для обоих признаков.
fig, ax = plt.subplots(figsize=(15, 10))
ax.bar(y_train.columns[0], scores['RandomForestRegressor']['rfr_r2_score_feature_1'], color = 'blue', width = 0.5)
ax.bar(y_train.columns[1], scores['RandomForestRegressor']['rfr_r2_score_feature_2'], color = 'green', width = 0.5)

#Визуализириуем качество обобщения модели на тестовых данных для каждого признака
fig, axs = plt.subplots(2, figsize=(25, 20))
for i in range(2):
    axs[i].plot(y_pred[i], label="Прогнозные значения", color='blue')
    axs[i].plot(y_test.iloc[:, i].values, label="Тестовые значения", color='green')
    axs[i].set(xlabel="Количество наблюдений")
    axs[i].set(ylabel=y_train.columns[i])
    axs[i].legend(loc='best')

"""k-ближайших соседей"""

#Регрессия на основе соседей может использоваться в случаях, когда метки данных являются непрерывными, 
# а не дискретными переменными. Метка, присвоенная точке запроса, вычисляется на основе среднего значения 
# меток ее ближайших соседей.

#Метод ближайших соседей также имеет гиперпараметр, который необходимо передать методу GridSearchCV в целях подбора его
# наилучшего значения.

KNeighbors = list(range(1, 31))
param_grid = {'n_neighbors': range(1, 31)}

clf = GridSearchCV(KNeighborsRegressor(), param_grid, cv=10, n_jobs=-1, verbose=1)

#Для первого признака
clf.fit(X_train, y_train.iloc[:, 0])
print(f"Лучшие параметры для предсказания признака 'Модуль упругости при растяжении' {clf.best_params_}")

#Для второго признака
clf.fit(X_train, y_train.iloc[:, 1])
print(f"Лучшие параметры для предсказания признака 'Прочность при растяжении, МПа' {clf.best_params_}")

#Наилучшими параметрами для 
# предсказания признака 'Модуль упругости при растяжении' являются:
#  -количество соседей: 10
# для предсказания признака 'Прочность при растяжении, МПа' являются:
#  -количество соседей: 4

y_pred = []
knn_scores = {}  #Список коэффициентов детерминации для k-ближайших соседей

#Обучаем модель для предсказания признака "Модуль упругости при растяжении"
knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train.iloc[:, 0])
y_pred.append(knn.predict(X_test))
r2_score_feature_1 = r2_score(y_test.iloc[:, 0], y_pred[0])
print(f'Коэффициент детерминации для признака "{y_train.columns[0]}": {r2_score(y_test.iloc[:, 0], y_pred[0])}')
knn_scores["knn_r2_score_feature_1"] = r2_score_feature_1

#Обучаем модель для предсказания признака "Прочность при растяжении, МПа"
knn = KNeighborsRegressor(n_neighbors=4)
knn.fit(X_train, y_train.iloc[:, 1])
y_pred.append(knn.predict(X_test))
r2_score_feature_2 = r2_score(y_test.iloc[:, 1], y_pred[1])
print(f'Коэффициент детерминации для признака "{y_train.columns[1]}": {r2_score(y_test.iloc[:, 1], y_pred[1])}')
knn_scores["knn_r2_score_feature_2"] = r2_score_feature_2

scores['KNeighborsRegressor'] = knn_scores

#Визуализируем полученный результат для обоих признаков.
fig, ax = plt.subplots(figsize=(15, 10))
ax.bar(y_train.columns[0], scores['KNeighborsRegressor']['knn_r2_score_feature_1'], color = 'blue', width = 0.5)
ax.bar(y_train.columns[1], scores['KNeighborsRegressor']['knn_r2_score_feature_2'], color = 'green', width = 0.5)

#Визуализириуем качество обобщения модели на тестовых данных для каждого признака
fig, axs = plt.subplots(2, figsize=(25, 20))
for i in range(2):
    axs[i].plot(y_pred[i], label="Прогнозные значения", color='blue')
    axs[i].plot(y_test.iloc[:, i].values, label="Тестовые значения", color='green')
    axs[i].set(xlabel="Количество наблюдений")
    axs[i].set(ylabel=y_train.columns[i])
    axs[i].legend(loc='best')

"""Выбор лучшей модели"""

#Выведем оценки для предсказаний двух целевых признаков для каждой из моделей, 
# где первым признаком является "Модуль упругости при растяжении", вторым - "Прочность при растяжении, МПа"
for key in sorted(scores):
    print(scores[key])

#Если бы моделей было больше, определить лучшую модель на глаз не представилось бы простой задачей. Найдем лучшую модель для 
# каждого из признаков путем визуализации оценок предсказаний.

#Промежуточные хранилища для разделения оценок по каждому из параметров в целях упрощения построения графиков.
feature_1 = {}
feature_2 = {}

#Заполняем промежуточные хранилища
#По каждой модели
for key in scores:
    
    #По каждому признаку
    for i, feature in enumerate(scores[key]):
        
        #Вырезаем название модели для удобочитаемости на графике
        modelName = feature[: feature.index('_')]
        
        #Для первого признака
        if i == 0:
            
            #Словарь оценок моделей для первого признака
            feature_1[modelName] = scores[key][feature]
            
        #Для второго признака
        elif i == 1:
            #Словарь оценок моделей для второго признака
            feature_2[modelName] = scores[key][feature]

#Создаем датафрем для визуализации разделения
feature_1 = pd.DataFrame.from_dict(feature_1, orient='index', 
                                    columns=["Оценка для признака 'Модуль упругости при растяжении'"])
feature_1

#Визуализируем оценки по каждой модели для первого признака
ax = feature_1.plot.bar(rot=0, color='darkblue', figsize=(20,15), width=0.5, xlabel='Название модели')
plt.legend(loc='lower right')
plt.xticks(rotation=45, ha='right')

#Очевидным является тот факт, что лучшей моделью для предсказания признака "Модуль упругости при растяжении" является
# модель случайного леса.

feature_2 = pd.DataFrame.from_dict(feature_2, orient='index', 
                                    columns=["Оценка для признака 'Прочность при растяжении'"])
feature_2

#Визуализируем оценки по каждой модели для второго признака
ax = feature_2.plot.bar(rot=0, color='darkblue', figsize=(20,15), width=0.5, xlabel='Название модели')
plt.legend(loc='lower right')
plt.xticks(rotation=45, ha='right')

#Так как с предсказанием второго признака модели справились практически одинаково,
# оптимальной моделью, для удобства будем считать также модель случайного леса.

"""Сохранение модели на диск"""

#Еще раз обучим каждую из моделей перед сохранением на диск.

#Так как данные, которые будут поступать в модель машинного обучения, встроенную в приложение будут являться "сырыми",
# а модель будет обучена на нормализованных данных, модель будет плохо предсказывать целевые признаки.
#Для решения этой проблемы необходимо создать рабочий поток обработки данных и обучения модели. Будем использовать
# метод make_pipeline. В качестве метода преобразования данных будем использовать метод StandardScaler() для 
# лучшей сходимости алгоритма на новых данных.

#С набором параметров для предсказания первого признака "Модуль упругости при растяжении"
pipe_rfr_elasticity = Pipeline([('preprocessing', StandardScaler()), 
                                ('regressor', RandomForestRegressor(n_estimators=500, max_depth=7, random_state=1))])
pipe_rfr_elasticity.fit(X_train, y_train.iloc[:, 0])

#С набором параметров для предсказания второго признака "Прочность при растяжении, МПа"
pipe_rfr_strength = Pipeline([('preprocessing', StandardScaler()), 
                                ('regressor', RandomForestRegressor(n_estimators=250, max_depth=10, random_state=1))])
pipe_rfr_strength.fit(X_train, y_train.iloc[:, 1])

#Проверяем работоспособность моделей для каждого из признаков
y_pred_elasticity_before = pipe_rfr_elasticity.predict(X_test)
y_pred_elasticity_before[0]

y_pred_strength_before = pipe_rfr_strength.predict(X_test)
y_pred_strength_before[0]

#Сохраняем модели на диск с помощью модуля pickle. Данный модуль не рекомендуется использовать в промышленных приложениях, 
# но так как задание является учебным, данный модуль можно использовать на свой "страх и риск" загрузить неликвидные
# или зараженные данные на диск рабочей машины.

#Сохраняем первую модель
with open('pipe_rfr_elasticity.pkl','wb') as outp:
    pickle.dump(pipe_rfr_elasticity, outp)
        
#Сохраняем вторую модель
with open('pipe_rfr_strength.pkl','wb') as outp:
    pickle.dump(pipe_rfr_strength, outp)

#Загружаем модели и проверяем их работоспособность
with open('pipe_rfr_elasticity.pkl', 'rb') as inp:
    pipe_rfr_elasticity = pickle.load(inp)
    
y_pred_elasticity_after = pipe_rfr_elasticity.predict(X_test)

#В данной строке мы проверяем каждое старое прогнозное значение с новым. Функция all возвращает true, если логическая маска
# целиком состоит из значений true. 
print((y_pred_elasticity_before==y_pred_elasticity_after).all())

with open('/models/pipe_rfr_strength.pkl', 'rb') as inp:
    pipe_rfr_strength = pickle.load(inp)

y_pred_strength_after = pipe_rfr_strength.predict(X_test)
print((y_pred_strength_before==y_pred_strength_after).all())

#Обе модели работают корректно после сохранения и загрузки в ноутбук с локального диска.

"""Нейросетевая регрессия"""

#По условию задачи, также, необходимо реализовать алгоритм на основе искусственных нейронных сетей.
# Нейронные сети обычно используют для классификации. Сигналы проходят через слои нейронов и обобщаются в один из 
# нескольких классов. Однако их можно адаптировать в регрессионные модели, если изменить последнюю функцию активации.
#Заменив последнюю функцию активации (выходной нейрон) линейной функцией активации, выходной сигнал можно отобразить 
# на множество значений, выходящих за пределы фиксированных классов.

#Для решения поставленной задачи с большим количеством регрессоров(независимых переменных) следует использовать полносвязную
# нейронную сеть, с целью попытаться выявить дополнительные связи между признаками, ввиду низкой коллинеарности между ними.

#Для построения полносвязной искусственной нейронной сети будем использовать библиотеку Keras.
model = Sequential()

#Так как преобразованные данные в датасете находятся в диапазоне [0..1], т.е. не имеют отрицательных значений, при построении
# архитектуры нейронной сети будем использовать активационную функцию ReLu.
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='softmax'))

#Выходной слой с одним линейным нейроном для решения задачи регресси
model.add(Dense(1, activation='linear'))

#Архитектура построенной нейронной сети
model.summary()

#Скомпилируем ИНС. В качестве функции потерь будем использовать Mean-Squared-Error(MSE) - среднюю квадратичную ошибку.
# Метрикой будет Mean-Absolute-Error(MAE) - средний модуль ошибки.
model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])

"""Обучение ИНС для целевого признака "Модуль упругости при растяжении"
"""

#Оценка обученной модели на тестовом наборе для первого целевого признака
result = model.evaluate(X_test, y_test.iloc[:, 0])
print(f"loss: {result[0]} | mae: {result[1]}")

#Обучаем модель для первого признака
history_first = model.fit(X_train, 
                    y_train.iloc[:, 0], 
                    epochs=100, 
                    validation_split=0.1, 
                    verbose=1)

#Делаем прогноз
y_pred = model.predict(X_test)
y_pred[0]

# Средний модуль отклонения
error = np.mean(abs(y_pred - np.ravel(y_test.iloc[:, 0])))
print(error)

#Визуализируем график потерь модели при обучении для первого признака
plt.figure(figsize = (15, 5))
plt.plot(history_first.history['loss'], color='blue')
plt.plot(history_first.history['val_loss'], color='green')
plt.title("Потери модели при обучении для признака 'Модуль упругости при растяжении'")
plt.ylabel('Значение ошибки')
plt.xlabel('Эпохи обучения')
plt.legend(['Обучающая выборка', 'Тестовая выборка'])
plt.show()

#Визуализириуем качество обобщения искусственной нейронной сети на тестовых данных для каждого признака
plt.figure(figsize = (15, 5))
plt.plot(y_pred, label="Прогнозные значения", color='blue')
plt.plot(y_test.iloc[:, 0].values, label="Тестовые значения", color='green')

#Рассчитаем коэффицент детерминации искусственной нейронной сети для первого признака
print(f'Коэффициент детерминации для признака "{y_train.columns[0]}": {r2_score(y_test.iloc[:, 0], y_pred)}')

#Сравним полученную оценку с оценками других моделей машинного обучения

#Создадим датафрейм на основе полученной оценки для искусственной нейронной сети по первому целевому признаку
NN = {'NN': r2_score(y_test.iloc[:, 0], y_pred)}

NN = pd.DataFrame.from_dict(NN, orient='index', 
                                    columns=["Оценка для признака 'Модуль упругости при растяжении'"])

NN

#Присоединяем датафрейм с оценкой для искусственной нейронной сети
feature_1_with_NN = pd.DataFrame()
frames = [feature_1, NN]
feature_1_with_NN = pd.concat(frames, ignore_index=False)

feature_1_with_NN

#Посмотрим информацию по данному датафрейму
feature_1_with_NN.describe()

#Среднее значение оценки равно 0.769

#Снова визуализируем оценки по каждой модели для первого признака
ax = feature_1_with_NN.plot.bar(rot=0, color='darkblue', figsize=(20,15), width=0.5, xlabel='Название модели')
plt.legend(loc='lower right')
plt.xticks(rotation=45, ha='right')

#Лидером по прежнему остается модель случайного леса.

"""Обучение ИНС для целевого признака "Прочность при растяжении"
"""

#Для оценки второго признака создадим отдельную архитектуру
model2 = Sequential()

model2.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model2.add(Dense(128, activation='relu'))
model2.add(Dense(64, activation='relu'))
model2.add(Dense(32, activation='relu'))
model2.add(Dense(16, activation='relu'))

model2.add(Dense(1, activation='linear'))

#Компилируем
model2.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])

# Оценка обученной модели на тестовом наборе для второго целевого признака
result = model2.evaluate(X_test, y_test.iloc[:, 1])
print(f"loss: {result[0]} | mae: {result[1]}")

#Обучаем модель для второго признака
history_second = model2.fit(X_train, 
                    y_train.iloc[:, 1], 
                    epochs=100, 
                    validation_split=0.1)

#Делаем прогноз
y_pred = model2.predict(X_test)
y_pred[0]

#Визуализируем график потерь модели при обучении для второго признака
plt.figure(figsize = (15, 5))
plt.plot(history_second.history['loss'], color='blue')
plt.plot(history_second.history['val_loss'], color='green')
plt.title("Потери модели при обучении для признака 'Прочность при растяжении'")
plt.ylabel('Значение ошибки')
plt.xlabel('Эпохи обучения')
plt.legend(['Обучающая выборка', 'Тестовая выборка'])
plt.show()

#Визуализириуем качество обобщения искусственной нейронной сети на тестовых данных для каждого признака
plt.figure(figsize = (15, 5))
plt.plot(y_pred, label="Прогнозные значения", color='blue')
plt.plot(y_test.iloc[:, 1].values, label="Тестовые значения", color='green')

#Рассчитаем коэффицент детерминации искусственной нейронной сети для первого признака
print(f'Коэффициент детерминации для признака "{y_train.columns[1]}": {r2_score(y_test.iloc[:, 1], y_pred)}')

#Сравним полученную оценку с оценками других моделей машинного обучения

#Создадим датафрейм на основе полученной оценки для искусственной нейронной сети по второму целевому признаку
NN = {'NN': r2_score(y_test.iloc[:, 1], y_pred)}

NN = pd.DataFrame.from_dict(NN, orient='index', 
                                    columns=["Оценка для признака 'Прочность при растяжении'"])

NN

#Присоединяем датафрейм с оценкой для искусственной нейронной сети
feature_2_with_NN = pd.DataFrame()
frames = [feature_2, NN]
feature_2_with_NN = pd.concat(frames, ignore_index=False)

feature_2_with_NN

#Посмотрим информацию по данному датафрейму
feature_2_with_NN.describe()

#Среднее значение оценки равно 0.955

#Снова визуализируем оценки по каждой модели для второго признака
ax = feature_2_with_NN.plot.bar(rot=0, color='darkblue', figsize=(20,15), width=0.5, xlabel='Название модели')
plt.legend(loc='lower right')
plt.xticks(rotation=45, ha='right')

#Искусственная нейронная сеть превосходит по качеству модель случайного леса.

#Сохраним обученную модель искусственной нейронной сети для предсказания второго целевого признака.
# Перед сохранением обучим модель еще раз.

#Создаем архитектуру
modelTOSave = Sequential()
modelTOSave.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
modelTOSave.add(Dense(128, activation='relu'))
modelTOSave.add(Dense(64, activation='relu'))
modelTOSave.add(Dense(32, activation='relu'))
modelTOSave.add(Dense(16, activation='relu'))
modelTOSave.add(Dense(1, activation='linear'))

#Компилируем
modelTOSave.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])

#Обучаем
modelTOSave.fit(X_train, y_train.iloc[:, 1], epochs=100, validation_split=0.1)

y_pred = modelTOSave.predict(X_test)
y_pred[0]

#Сохраняем обученную модель
modelTOSave.save("NN_strength")

#Загружаем модель и проверяем ее работоспособность
loaded_model = keras.models.load_model("models/NN_strength")

y_pred = modelTOSave.predict(X_test)
y_pred_loaded = loaded_model.predict(X_test)
print((y_pred==y_pred_loaded).all())

#Загружаемая модель работает как и ожидалось, следовательно, она готова к реализации в приложении.

"""   На данном этапе исследовательскую работу можно считать завершенной. По результатам исследования удалось выяснить, что входной датасет имеет некоторое количество выбросов, что означает необходимость тщательной проверки входных данных на превышение граничных значений в дальнейшем. Результат обучения моделей на первом целевом признаке - 'Модуль упругости при растяжении' можно считать удовлетворительным, но недостаточным для использования в промышленных целях, так как модели хоть и предсказывают значение признака лучше, чем простое подбрасывание монеты, но делают это плохо(Средний коэффициент детерминации равен 0.77, где максимальное значение равняется 1). Обучившись предсказывать второй признак, каждая из моделей показала хороший результат, со средним значением коэффицента равным 0.95. Из этого следует, что для применения в промышленной среде можно использовать модель предсказания второго целевого признака - "Прочность при растяжении". 
   
   На основании созданных и обученных моделей будет реализовано веб-приложение для предсказания двух приведенных выше целевых признаков, с интуитивно-понятным пользователю интерфейсом.
"""